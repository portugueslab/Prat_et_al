{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flammkuchen as fl\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import io\n",
    "import seaborn as sns\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from luminance_analysis import PooledData, traces_stim_from_path\n",
    "import os\n",
    "\n",
    "plt.style.use(\"figures.mplstyle\")\n",
    "cols = sns.color_palette()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig_fold = None # Path(r\"J:\\_Shared\\GC_IO_luminance\\figures\\fig7\\src\")\n",
    "\n",
    "fig_fold = Path(r\"C:\\Users\\otprat\\Documents\\figures\\luminance\\manuscript_figures\\fig7_v2\")\n",
    "if not os.path.isdir(fig_fold):\n",
    "    os.mkdir(fig_fold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_path = Path(r\"\\\\FUNES\\Shared\\experiments\\E0032_luminance\\neat_exps\")\n",
    "# master_path = Path(r\"J:\\_Shared\\GC_IO_luminance\\data\\neat_exps\")\n",
    "# master_path = Path(r\"/Users/luigipetrucco/Desktop/data_dictionaries/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from luminance_analysis.utilities import deconv_resamp_norm_trace, reliability, nanzscore, get_kernel\n",
    "from skimage.filters import threshold_otsu\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, cut_tree, to_tree, set_link_color_palette\n",
    "from luminance_analysis.plotting import plot_clusters_dendro, shade_plot, add_offset_axes, make_bar\n",
    "from luminance_analysis.clustering import cluster_id_search, find_trunc_dendro_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<luminance_analysis.FishData object at 0x0000017158A15C48>, <luminance_analysis.FishData object at 0x0000017158A15CC8>, <luminance_analysis.FishData object at 0x0000017158A26688>, <luminance_analysis.FishData object at 0x0000017158A2AE48>, <luminance_analysis.FishData object at 0x0000017158A33648>]\n",
      "[<luminance_analysis.FishData object at 0x0000017158A1AC48>, <luminance_analysis.FishData object at 0x0000017158A1ABC8>, <luminance_analysis.FishData object at 0x0000017158A26708>, <luminance_analysis.FishData object at 0x0000017158A2A8C8>, <luminance_analysis.FishData object at 0x0000017158A33648>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\tools\\miniconda3\\envs\\rplab\\lib\\site-packages\\numpy\\lib\\function_base.py:2642: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[:, None]\n",
      "C:\\tools\\miniconda3\\envs\\rplab\\lib\\site-packages\\numpy\\lib\\function_base.py:2643: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[None, :]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<luminance_analysis.FishData object at 0x0000017100C9CF88>, <luminance_analysis.FishData object at 0x0000017100CA8048>, <luminance_analysis.FishData object at 0x0000017100CB5608>, <luminance_analysis.FishData object at 0x0000017100CC3BC8>, <luminance_analysis.FishData object at 0x0000017100CAE1C8>]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "446c4d091b1345f98397aa46c129f07b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<luminance_analysis.FishData object at 0x0000017100E69988>, <luminance_analysis.FishData object at 0x0000017100E69A08>, <luminance_analysis.FishData object at 0x0000017100E6E088>, <luminance_analysis.FishData object at 0x0000017100E728C8>, <luminance_analysis.FishData object at 0x0000017100E77148>]\n",
      "[<luminance_analysis.FishData object at 0x0000017100E6ABC8>, <luminance_analysis.FishData object at 0x0000017100E6AB48>, <luminance_analysis.FishData object at 0x0000017100E6E5C8>, <luminance_analysis.FishData object at 0x0000017100CC8808>, <luminance_analysis.FishData object at 0x0000017100E7F3C8>]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d60956a4576412aaaabb0726d4fa2ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<luminance_analysis.FishData object at 0x00000171008BBEC8>, <luminance_analysis.FishData object at 0x00000171008BD208>, <luminance_analysis.FishData object at 0x00000171008C2388>, <luminance_analysis.FishData object at 0x00000171008C6508>, <luminance_analysis.FishData object at 0x00000171008CB848>]\n",
      "[<luminance_analysis.FishData object at 0x00000171008BDF08>, <luminance_analysis.FishData object at 0x00000171008BDD88>, <luminance_analysis.FishData object at 0x00000171008C6348>, <luminance_analysis.FishData object at 0x00000171008D7088>, <luminance_analysis.FishData object at 0x00000171008CB588>]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0150acf798674994998d0476a17303b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tau_6f = 5\n",
    "tau_6s = 8\n",
    "ker_len = 20\n",
    "normalization = \"zscore\"\n",
    "protocol = 'steps'\n",
    "\n",
    "brain_regions_list = [\"GC\", \"IO\", \"PC\"]\n",
    "tau_list = [tau_6f, tau_6f, tau_6s]\n",
    "n_cluster_list = [8, 6, 8]\n",
    "nan_thr_list = [0, 1, 1]\n",
    "\n",
    "data_dict = {k:{} for k in brain_regions_list}\n",
    "\n",
    "#load stimulus of GCs and use it as a the reference for time array and stimulus array:\n",
    "stim_ref = PooledData(path = master_path / protocol / \"GC\").stimarray_rep\n",
    "\n",
    "for brain_region, tau, n_cluster, nan_thr in zip(brain_regions_list, tau_list, \n",
    "                                                 n_cluster_list, nan_thr_list):\n",
    "    #Load data :\n",
    "    path = master_path / protocol / brain_region\n",
    "    stim, traces, meanresps = traces_stim_from_path(path)\n",
    "\n",
    "    # Mean traces, calculate reliability index :\n",
    "    rel_idxs = reliability(traces)\n",
    "    \n",
    "    # Find threshold from reliability histogram...\n",
    "    rel_thr = threshold_otsu(rel_idxs[~np.isnan(rel_idxs)])\n",
    "\n",
    "    # ...and load again filtering with the threshold:\n",
    "    _, traces, meanresps = traces_stim_from_path(path, resp_threshold=rel_thr, nanfraction_thr=nan_thr)\n",
    "\n",
    "    # Hierarchical clustering:\n",
    "    linked = linkage(meanresps, 'ward')\n",
    "    \n",
    "    # Truncate dendrogram at n_cluster level:\n",
    "    plt.figure(figsize=(0.1, 0.1))  \n",
    "    dendro = dendrogram(linked, n_cluster, truncate_mode =\"lastp\")\n",
    "    plt.close()\n",
    "    cluster_ids = dendro[\"leaves\"]\n",
    "    labels = find_trunc_dendro_clusters(linked, dendro) \n",
    "    \n",
    "    # Deconvolution, resampling / normalization:\n",
    "    deconv_meanresps = np.empty((meanresps.shape[0], stim_ref.shape[0]))\n",
    "    resamp_meanresps = np.empty((meanresps.shape[0], stim_ref.shape[0]))\n",
    "    for roi_i in range(deconv_meanresps.shape[0]):\n",
    "        deconv_meanresps[roi_i, :] = deconv_resamp_norm_trace(meanresps[roi_i, :], stim[:, 0],\n",
    "                                                                stim_ref[:, 0], tau, ker_len,\n",
    "                                                                smooth_wnd=4,\n",
    "                                                                normalization=normalization)\n",
    "        resamp_meanresps[roi_i, :] = deconv_resamp_norm_trace(meanresps[roi_i, :], stim[:, 0],\n",
    "                                                                stim_ref[:, 0], None, ker_len,\n",
    "                                                                smooth_wnd=4,\n",
    "                                                                normalization=normalization)\n",
    "    \n",
    "    cluster_resps = np.empty((n_cluster, stim_ref.shape[0]))\n",
    "    for clust_i in range(n_cluster):\n",
    "        cluster_resp = np.nanmean(deconv_meanresps[labels==clust_i, :], 0)  # average cluster responses\n",
    "        cluster_resps[clust_i, :] = nanzscore(cluster_resp)  # normalize\n",
    "\n",
    "\n",
    "    # Add everything to dictionary:\n",
    "    data_dict[brain_region][\"linkage_mat\"] = linked\n",
    "    data_dict[brain_region][\"clust_labels\"] = labels\n",
    "    #data_dict[brain_region][\"raw_mn_resps\"] = meanresps\n",
    "    data_dict[brain_region][\"traces\"] = traces\n",
    "    #data_dict[brain_region][\"deconv_mn_resps\"] = deconv_meanresps\n",
    "    #data_dict[brain_region][\"resamp_mn_resps\"] = resamp_meanresps\n",
    "    data_dict[brain_region][\"rel_idxs\"] = rel_idxs[rel_idxs > rel_thr]\n",
    "    data_dict[brain_region][\"rel_thr\"] = rel_thr\n",
    "    data_dict[brain_region][\"clust_resps\"] = cluster_resps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description of the model\n",
    "\n",
    "\n",
    "The goal of the model is to reconstruct the activity of PCs based on the activity observed for GCs and IONs.\n",
    "The function that we will use to model a PC is the following:\n",
    "$$ trace_{PC}^i = o^i + clusters_{GC} * w^i_{GC} + clusters_{IO} * w^i_{IO}$$\n",
    "\n",
    "\n",
    " - $trace_{PC}^i$ is the $i^{th}$ PC cell trace;\n",
    " - $o^i$ is an offset term:\n",
    " - $clusters_{GC}$, $clusters_{IO}$ are matrices with the average activation of all GC&IO clusters;\n",
    " - $w^i_{GC}$, $w^i_{IO}$ are weights vectors for each of the GC and IO cluster. We will allow positive and negative $w^i_{GC}$, but only positive $w^i_{IO}$. This is a quite safe assumption considering the known PC physiology. \n",
    "\n",
    "\n",
    "### Approach\n",
    "\n",
    "Here is a summary of the modelling approach:\n",
    "- **Create a panel of regressors**:\n",
    "    - Calculate clusters of GC and IO responses; \n",
    "    - Deconvolve average response of each cluster with 6fe05 kernel;\n",
    "    - Reconvolve it with 6s kernel;\n",
    "    - Normalize it to be  > 0, and with integral = 1.\n",
    "\n",
    "\n",
    "- **Clean up PC traces**:\n",
    "    - For each cell, take raw fluorescence if valid trials, and zscore them on a trial-to-trial base (for changes in offset F across planes);\n",
    "    - concatenate repetitions;\n",
    "    - high-pass filter them with very low cutoff freq (1/80 Hz) to remove slow fluctuations;\n",
    "    - smooth them with a 3 pts mean boxcar rolling window;\n",
    "    \n",
    "    \n",
    "- **Split fit and test data**:\n",
    "    - Randomly pick from each cell:\n",
    "        - 2 repetitions that will be left out for the analysis (**test traces**);\n",
    "        - 4 repetitions (or more, if there are more planes) that will be used for finding the regularization term and for the actual fitting (**fit traces**);\n",
    "\n",
    "\n",
    "- **Define boundaries, cost function and regularization function**:\n",
    "    - Parameters to be optimized are:\n",
    "        - *offset*: a constant term, bound to be between -5 and 5;\n",
    "        - *coefs_GC*: coefficients for GC regressors, bound to be between -1000 and 1000 (the large difference comes from the different normalizations applied on regressors -norm- and on trace - Z scoring)\n",
    "        - *coefs_IO*: coefficients for GC regressors, bound to be between 0 and 1000, as we have good reasons to postulate that IO contributions are strictly positive\n",
    "        - cost function: L2 distance to target trace;\n",
    "        - regularization function: L1 (sum of absolute value of parameters)\n",
    "        \n",
    "        \n",
    "- **Find regularization parameter**:\n",
    "    - Find regularization parameter (leave one out cross-validation):\n",
    "    - For each lambda parameter, train the model on all the fit traces but one (**train traces**);\n",
    "    - Then, calculate the cost of the fit on the one trace that was left out (**validation trace**);\n",
    "    - Do this iterating over all possible combinations of n-1 and 1 traces;\n",
    "    - Calculate average cross over all combinations, over all cells;\n",
    "\n",
    "\n",
    "- **Fit the trace**:\n",
    "    - Use the resulting regularization term for fitting the fit repetitions, and use the obtained coefficients for plots / further analyses on the test repetitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a panel of regressors:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create regressor panel, making traces non-0 and with integral equal to one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gc_clust = data_dict[\"GC\"][\"clust_resps\"].shape[0]\n",
    "n_io_clust = data_dict[\"IO\"][\"clust_resps\"].shape[0]\n",
    "regressors_mat = np.concatenate([data_dict[\"GC\"][\"clust_resps\"], data_dict[\"IO\"][\"clust_resps\"]])\n",
    "\n",
    "# Reconvolve and normalize regressors:\n",
    "for i in range(regressors_mat.shape[0]):\n",
    "    reconvolved = np.convolve(regressors_mat[i, :], get_kernel(ker_len=100, tau=tau_6s))[:regressors_mat.shape[1]]\n",
    "    \n",
    "    # Make strictly positive and with integral == 1:\n",
    "    reconvolved -= np.min(reconvolved)  # offset at 0\n",
    "    regressors_mat[i, :] = reconvolved / np.sum(reconvolved)\n",
    "\n",
    "# Arbitrary cluster names (do we need them?):\n",
    "gc_cluster_names = ['ON 1', 'ON 2', 'ON abs', 'ON inter.1', 'ON inter.2', 'OFF 1', 'OFF 2', 'OFF inter.']\n",
    "io_cluster_names = ['Onset', 'ON max', 'Offset 1', 'Offset 2', 'On inter.', 'Offset 3']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the regressors panel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the regressor panel:\n",
    "def reg_panel_plot(regressors_mat, figure=None, ax=None, frame=None):\n",
    "    if figure is None:\n",
    "        figure = plt.figure(figsize=(3, 3))\n",
    "\n",
    "    offset = 0.01\n",
    "    if ax is None:\n",
    "        ax = add_offset_axes(figure, (0., 0., 1, 1), frameon=False, frame=frame)\n",
    "    cols = sns.color_palette()\n",
    "    for i, col in enumerate([cols[0], ]*n_gc_clust + [cols[1], ]*n_io_clust):\n",
    "        ax.fill_between(stim[:, 0], np.zeros(stim[:, 0].shape) - i*offset, \n",
    "                        regressors_mat[i, :] - i*offset, color=col)\n",
    "        \n",
    "    ax.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87d40117ed464dda904dff0a60e40527",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reg_panel_plot(regressors_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform PCA on regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24b726119065468a92e9965f2ddcbe28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Perform PCA\n",
    "pca = PCA(n_components=14) #Start by looking at the firts 25 PCs.\n",
    "pca.fit(regressors_mat)\n",
    "\n",
    "#Plot the cumulative explained variance by the main PCs.\n",
    "x=np.arange(0,14,1)\n",
    "expl_var=np.cumsum(pca.explained_variance_ratio_)\n",
    "fig = plt.figure(figsize=(3,3))\n",
    "plt.plot(x, expl_var)\n",
    "plt.xlabel('Principal Components')\n",
    "plt.ylabel('Explained Variance')\n",
    "plt.grid()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define number of principal components based on the explained variance per PC above\n",
    "n_components = 6\n",
    "pca=PCA(n_components=n_components)\n",
    "regressors_pca=pca.fit_transform(regressors_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcs_mat = np.full((n_components, regressors_mat.shape[1]), np.nan)\n",
    "\n",
    "for pc in range(n_components):\n",
    "    zeros = np.zeros(n_components)\n",
    "    zeros[pc] = 1\n",
    "    pcs_mat[pc, :] = pca.inverse_transform(zeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b80fc87ea3c41f3836ba9f7924594dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Timepoints')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pcs_fig = plt.figure()\n",
    "\n",
    "for pc in range(n_components):   \n",
    "    plt.plot(pcs_mat[pc, :] - .25*pc, c=sns.color_palette()[2])\n",
    "    \n",
    "plt.yticks([])\n",
    "plt.ylabel('PCs')\n",
    "plt.xlabel('Timepoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if fig_fold is not None:\n",
    "    pcs_fig.savefig(fig_fold / \"principal_components.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b96b0127bbe744928849b324e2ff037d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot PC coefficients for regressors\n",
    "gc_regs = np.arange(n_gc_clust)\n",
    "io_regs = np.arange(n_io_clust)+n_gc_clust\n",
    "\n",
    "reg_pcs_fig, axes = plt.subplots(n_components, n_components, figsize=(7, 7), sharex=True, sharey=True)\n",
    "\n",
    "for i in range(n_components):\n",
    "    for j in range(n_components):\n",
    "        if j>=i:\n",
    "            axes[i, j].axis('off')\n",
    "        else:\n",
    "            for gc_reg in gc_regs:\n",
    "                axes[i, j].plot([0, regressors_pca[gc_reg, j]], [0, regressors_pca[gc_reg, i]], c=sns.color_palette()[0])\n",
    "            for io_reg in io_regs:\n",
    "                axes[i, j].plot([0, regressors_pca[io_reg, j]], [0, regressors_pca[io_reg, i]], c=sns.color_palette()[1])\n",
    "            axes[i, j].set_xticks([])\n",
    "            axes[i, j].set_yticks([])\n",
    "            axes[i, j].axvline(0, ls='--', c='white', alpha=.2)\n",
    "            axes[i, j].axhline(0, ls='--', c='white', alpha=.2)\n",
    "        \n",
    "for i in range(n_components):\n",
    "    axes[i, 0].set_ylabel('PC{}'.format(i+1))\n",
    "    \n",
    "for j in range(n_components):\n",
    "    axes[-1, j].set_xlabel('PC{}'.format(j+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if fig_fold is not None:\n",
    "    reg_pcs_fig.savefig(fig_fold / \"regressor_PCs.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up PC traces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from luminance_analysis.utilities import smooth_traces, butter_highpass_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_cell_rep_block(cellmat, cutoff=1/80, smooth_wnd=3):\n",
    "    \"\"\" Filter traces from the raw traces block.\n",
    "    Return a repetitions block containing only the valid repetitions.\n",
    "    \"\"\"\n",
    "    cutoff = 1 / 80\n",
    "    dt = stim[1, 0]\n",
    "    \n",
    "    # Select entries with valid numbers in the repetition matrix:\n",
    "    cellmat = cellmat[:, ~np.isnan(cellmat).all(0)].copy()\n",
    "    \n",
    "    # zscore repetition-wise, important for ROIs spanning more than one plane\n",
    "    cellmat = (cellmat - np.nanmean(cellmat, 0)) / np.nanstd(cellmat, 0)\n",
    "    \n",
    "    # concatenate, highpass filter with very low cutoff, and smooth:\n",
    "    trace = np.concatenate(cellmat.T, 0)\n",
    "    filtered = butter_highpass_filter(trace, cutoff, 1 / dt)  # filt trace\n",
    "    filtered = smooth_traces(filtered[np.newaxis, :], win=3, method=\"mean\")[0, :]  # smooth\n",
    "    filtered[np.isnan(filtered)] = 0\n",
    "    \n",
    "    # reshape in original form, and zscore again after filtering and smoothing:\n",
    "    reshaped = filtered.reshape(cellmat.T.shape).T  \n",
    "    reshaped = (reshaped - np.nanmean(reshaped, 0))/np.nanstd(reshaped, 0)\n",
    "    \n",
    "    return reshaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup parameters:\n",
    "cutoff_hz = 1 / 80  # long cutoff for highpass filter - remove long fluctuations in PC signal\n",
    "smooth_wnd = 3  # smoothing window to reduce noise. Our data is sampled ar around 0.25 seconds\n",
    "\n",
    "raw_traces = data_dict[\"PC\"][\"traces\"]  # raw PC fluorescences\n",
    "rel_idxs = data_dict[\"PC\"][\"rel_idxs\"]  # reliability indexes for each PC cell\n",
    "\n",
    "n_rois = raw_traces.shape[0]  # number of ROIs\n",
    "n_rep_timepts = raw_traces.shape[1]  # timepoints per repetition\n",
    "n_reps_max = raw_traces.shape[2]  # maximum number of repetitions in a cell\n",
    "\n",
    "# Find number of valid repetitions for each cell:\n",
    "n_valid_reps = (~np.isnan(raw_traces).all(1)).sum(1)\n",
    "\n",
    "# Clean up:\n",
    "clean_traces = np.full(raw_traces.shape, np.nan)\n",
    "for i_roi in range(n_rois):\n",
    "    clean_block = filter_cell_rep_block(raw_traces[i_roi, :, :], cutoff=cutoff_hz, smooth_wnd=smooth_wnd)\n",
    "    clean_traces[i_roi, :, :n_valid_reps[i_roi]] = clean_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a trace panel:\n",
    "def little_trace_plot(clean_traces, i=0, figure=None, ax=None, frame=None):\n",
    "    if figure is None:\n",
    "        figure = plt.figure(figsize=(3, 1))\n",
    "\n",
    "    if ax is None:\n",
    "        ax = add_offset_axes(figure, (0., 0., 1, 1), frameon=False, frame=frame)\n",
    "    offset = 0.01\n",
    "    plt.plot(clean_traces[i, :, :1].T.flatten())\n",
    "    ax.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85d43e2dda8b4a97a325c4ac3ad9519f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "little_trace_plot(clean_traces, i=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(672, 6, 48)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Transform Purkinje cell traces to the regressor PC space\n",
    "traces_pca = np.full((clean_traces.shape[0], n_components, clean_traces.shape[2]), np.nan)\n",
    "\n",
    "for roi in range(clean_traces.shape[0]): # For now, we iterate in such a way to deal with NaNed unexisting repetitions\n",
    "    for rep in range(clean_traces.shape[2]):\n",
    "        try:\n",
    "            traces_pca[roi, :, rep] = pca.transform(clean_traces[roi, :, rep].reshape(1, -1))\n",
    "        except ValueError:\n",
    "            pass\n",
    "        \n",
    "traces_pca.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will see how much GC and IO clusters contribute to individual Purkinje cell responses without the need to fit anything."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do so, we will start by calculating the average response for each Purkinje cell trace, normalizing it in the same way we normalized the GC and IO regressors, and projecting the traces to that same pC space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average Purkinje cell traces\n",
    "mean_traces = np.nanmean(clean_traces, 2)\n",
    "\n",
    "# Make strictly positive and with integral == 1:\n",
    "for roi in range(mean_traces.shape[0]):\n",
    "    roi_trace = mean_traces[roi, :]\n",
    "    roi_trace -= np.min(roi_trace)  # offset at 0\n",
    "    mean_traces[roi, :] = roi_trace / np.sum(roi_trace)\n",
    "\n",
    "# And transform to regressor PC space\n",
    "mean_traces_pca = pca.transform(mean_traces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_pc_similarity(reg_pcs, traces_pcs):\n",
    "    \n",
    "    #Calculate average and normalized std of regressor PCs\n",
    "    reg_mean = np.nanmean(reg_pcs, 0)\n",
    "    reg_std = np.nanstd(reg_pcs, 0)\n",
    "    reg_std_norm = reg_std/np.abs(reg_mean)\n",
    "    \n",
    "    #For each ROI, calculate the absolute difference between its loads and the average ones from the regressors, multiply them by the relative weight attributed to each PC (inverse of normalized std), and sum them up\n",
    "    like_idx = np.sum(np.abs(traces_pcs-reg_mean)*(1/reg_std_norm), 1)\n",
    "    \n",
    "    return 1/like_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc_like_index = calculate_pc_similarity(regressors_pca[gc_regs], mean_traces_pca)\n",
    "io_like_index = calculate_pc_similarity(regressors_pca[io_regs], mean_traces_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc_io_idx = (gc_like_index-io_like_index)/(gc_like_index+io_like_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6ee29df05e4433ba83ee3b4882c13fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Counts')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_distr_fig = plt.figure()\n",
    "plt.hist(gc_io_idx, bins=50, color=sns.color_palette()[2]);\n",
    "plt.axvline(0, ls='--', c='black')\n",
    "plt.xlabel('GC/IO index')\n",
    "plt.ylabel('Counts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "if fig_fold is not None:\n",
    "    idx_distr_fig.savefig(fig_fold / \"gc_io_idx_hist.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a IO-GC colormap\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "colors = [sns.color_palette()[1], (.72,)*3, sns.color_palette()[0]]\n",
    "n_bins = 200\n",
    "cmap_name = 'contributions_cmap'\n",
    "custom_cmap = LinearSegmentedColormap.from_list(cmap_name, colors, N=n_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_idx = data_dict[\"PC\"][\"rel_idxs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b2a09cd99094b4ebf6e347d03eba58b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'GC/IO index')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc_io_idx_fig = plt.figure()\n",
    "plt.scatter(gc_io_idx, rel_idx, c=gc_io_idx, cmap=custom_cmap, vmin=-1, vmax=1)\n",
    "plt.axvline(0, c='black', zorder=-100, ls='--')\n",
    "\n",
    "plt.ylabel('Reliability index')\n",
    "plt.xlabel('GC/IO index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "if fig_fold is not None:\n",
    "    gc_io_idx_fig.savefig(fig_fold / \"gc_io_idx_rel.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_pc_corr = np.full((regressors_mat.shape[0], n_components), np.nan)\n",
    "\n",
    "for reg in range(regressors_mat.shape[0]):\n",
    "    for pc in range(pcs_mat.shape[0]):\n",
    "        reg_pc_corr[reg, pc] = np.corrcoef(regressors_mat[reg, :], pcs_mat[pc, :])[0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_gc_likeness = np.abs(reg_pc_corr)[gc_regs].sum(0)\n",
    "pc_io_likeness = np.abs(reg_pc_corr)[io_regs].sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_gc_contr = np.full(n_components, np.nan)\n",
    "pc_io_contr = np.full(n_components, np.nan)\n",
    "\n",
    "for pc in range(n_components):\n",
    "    pc_gc_contr[pc] = pc_gc_likeness[pc]/(pc_gc_likeness[pc]+pc_io_likeness[pc])\n",
    "    pc_io_contr[pc] = pc_io_likeness[pc]/(pc_gc_likeness[pc]+pc_io_likeness[pc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc_contribution = np.full_like(mean_traces, np.nan)\n",
    "io_contribution = np.full_like(mean_traces, np.nan)\n",
    "\n",
    "for roi in range(mean_traces_pca.shape[0]):\n",
    "    \n",
    "    gc_coef = mean_traces_pca[roi, :] * pc_gc_contr\n",
    "    io_coef = mean_traces_pca[roi, :] * pc_io_contr\n",
    "    \n",
    "    gc_weighted_pcs = pcs_mat * gc_coef[:, None]\n",
    "    io_weighted_pcs = pcs_mat * io_coef[:, None]\n",
    "    \n",
    "    gc_contribution[roi, :] = np.sum(np.abs(gc_weighted_pcs), 0)\n",
    "    io_contribution[roi, :] = np.sum(np.abs(io_weighted_pcs), 0)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_contributions_mat = (gc_contribution-io_contribution) / (gc_contribution+io_contribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20a285e3ee054e1c8e86d5bc74e8c1c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Text(0, -0.03, 'IO contributions dominates')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_contr_fig, ax = plt.subplots(figsize=(10, 3))\n",
    "\n",
    "# Find quartiles:\n",
    "median_contr = np.nanmedian(time_contributions_mat, 0)\n",
    "low_quart_contr = np.nanquantile(time_contributions_mat, 0.25, axis=0)\n",
    "high_quart_contr = np.nanquantile(time_contributions_mat, 0.75, axis=0)\n",
    "\n",
    "# Plot trace and filling:\n",
    "i_col=5\n",
    "ax.plot(stim[:, 0], median_contr, color=sns.color_palette()[i_col])\n",
    "ax.fill_between(stim[:,0], low_quart_contr, high_quart_contr, facecolor=sns.color_palette()[i_col], \n",
    "                     alpha=.3, zorder=100, edgecolor=None)\n",
    "shade_plot(stim, ax, shade_range=(0.75, 0.98))\n",
    "ax.axhline(0, c = (0.3,)*3, zorder=1, ls='--')\n",
    "ax.set_ylim(-.5, .5)\n",
    "ax.set_xlim(0, stim[-1, 0])\n",
    "\n",
    "make_bar(ax, (stim[-1, 0]-10, stim[-1, 0]), label=\"10 s\")\n",
    "ax.set_ylabel('Contribution ratio')\n",
    "ax.text(0, 1.03, 'GC contributions dominates', fontsize=7, color=sns.color_palette()[0], transform=ax.transAxes, va='center')\n",
    "ax.text(0, -.03, 'IO contributions dominates', fontsize=7, color=sns.color_palette()[1], transform=ax.transAxes, va='center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "if fig_fold is not None:\n",
    "    time_contr_fig.savefig(fig_fold / \"time_contribution.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separate testing and training traces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle, seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix randomness for reproducibility:\n",
    "np.random.seed(572704)\n",
    "seed(572704)\n",
    "\n",
    "# Generate list of 2 indexes for each cell which will be used to keep traces out for the testing part.\n",
    "# Generate randomly indexes for test and train set of traces:\n",
    "n_test_reps = 3\n",
    "\n",
    "test_idxs = []\n",
    "fit_idxs = []\n",
    "for n_resps in n_valid_reps:\n",
    "    idxs = np.arange(n_resps)  # possible repetitions indexes\n",
    "    shuffle(idxs)  # shuffle index list\n",
    "    test_idxs.append(idxs[:n_test_reps])  # test idxs will be the first 2\n",
    "    fit_idxs.append(idxs[n_test_reps:])  # the rest goes for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define boundaries, function & regularisation/cost functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Functions for the regression.\n",
    "\n",
    "# This is the the main function that we actually use to describe PC activity:\n",
    "def offset_cluster_combine(coefs, regressors):\n",
    "    \"\"\" Compute trace from offset/coefficients and regressors.\n",
    "    It assumes coefs and regressors for GC and IO are all concatenated,\n",
    "    and first element of coefs array is the offset.\n",
    "    \"\"\"\n",
    "    return coefs[0] + np.sum(coefs[1:] * regressors, 1)  # first term is baseline\n",
    "\n",
    "def cost_func(fit_coefs, regressors, trace2fit, model):\n",
    "    \"\"\" Cost function: sum of squares.\n",
    "    \"\"\"\n",
    "    diff = trace2fit - model(fit_coefs, regressors)\n",
    "    return np.sum(diff**2) / trace2fit.shape[0]\n",
    "\n",
    "def reg_func(fit_coefs, reg_coef=0):\n",
    "    \"\"\" Regularization function: sum of absolute coefs values.\n",
    "    Does not regularize the offset, so first term is excluded:\n",
    "    \"\"\"\n",
    "    return np.sum(np.abs(fit_coefs[1:]))\n",
    "\n",
    "def minimization_func(fit_coefs, regressors, trace2fit, model, cost_func, reg_func, reg_lamda=0):\n",
    "    \"\"\"Full function to minimize, including cost and regularization terms.\n",
    "    \"\"\"\n",
    "    return cost_func(fit_coefs, regressors, trace2fit, model) + reg_func(fit_coefs) * reg_lamda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set starting values and bounds for the offset (the +1 below) and the coefficients.\n",
    "\n",
    "# Initial guesses:\n",
    "coefs_init_guess = np.zeros(regressors_mat.shape[0] + 1) \n",
    "\n",
    "# We know that IO input can only positively contribute to PC activity, \n",
    "# so we set IO coefficients to be positive:\n",
    "w_bound = 100  # bound for regressors weights (high b/c of normalization differences) \n",
    "off_bound = 5  # bound for the offset\n",
    "coefs_bounds =[(-off_bound, off_bound)] + \\\n",
    "              [(-w_bound, w_bound) for _ in range(n_gc_clust)] + \\\n",
    "              [(-w_bound, w_bound) for _ in range(n_io_clust)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate regularization parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we test for the optimal regularization lambda. As this parameter search can take quite long, you can just skip the section and execute from the next block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeaveOneOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare a concatenation of regressors long enough to fit the longest possible trace:\n",
    "regressors_concat = np.concatenate([regressors_mat,]*(n_reps_max - n_test_reps), 1).T\n",
    "\n",
    "# as the cost of a fit with all 0 coefficients is 1 - the std of the trace (which is zscored),\n",
    "# this will be our estimation of the cost that we use to decide the regularization lambda range \n",
    "# (2 orders of mag below and above the expected cost):\n",
    "reg_lambda_arr = np.insert(10**np.arange(-7., -2, 0.35), 0, 0)\n",
    "\n",
    "# Initialise empty matrices for storing the costs and the lambda parameters for all left-one-out fits:\n",
    "n_lambdas = reg_lambda_arr.shape[0]\n",
    "costs = np.full((n_rois, n_lambdas, n_reps_max - n_test_reps), np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "# Use scikit learn leave-one-out iterator:\n",
    "loo = LeaveOneOut()\n",
    "\n",
    "n_downsample = 1  # skip cells if we are testing. Otherwise, set to 1\n",
    "\n",
    "if n_downsample == 1 and (Path(\"\") / \"regularization_costs.h5\").exists():\n",
    "    costs = fl.load(\"regularization_costs.h5\")[\"costs\"]\n",
    "else:\n",
    "    for i_roi in range(0, n_rois, n_downsample):\n",
    "        if np.mod(i_roi, 50) == 0:\n",
    "            print(i_roi)\n",
    "        roi_fit_idxs = fit_idxs[i_roi]\n",
    "\n",
    "        # Hyperparameter grid search:\n",
    "        for i_lambda, reg_lambda in enumerate(reg_lambda_arr):\n",
    "\n",
    "            # Leave-one-out validation:\n",
    "            for i_loo, (idxs_train, idxs_valid) in enumerate(loo.split(roi_fit_idxs)):\n",
    "                roi_train_trace = clean_traces[i_roi, :, roi_fit_idxs[idxs_train]].flatten()\n",
    "                roi_valid_trace = clean_traces[i_roi, :, roi_fit_idxs[idxs_valid[0]]]\n",
    "                res = optimize.minimize(minimization_func, method='SLSQP', \n",
    "                                        args=(regressors_concat[:roi_train_trace.shape[0], :], \n",
    "                                              roi_train_trace, offset_cluster_combine, \n",
    "                                              cost_func, reg_func, reg_lambda),\n",
    "                                        x0=coefs_init_guess, bounds=coefs_bounds)\n",
    "\n",
    "                costs[i_roi, i_lambda, i_loo] = cost_func(res.x, regressors_concat[:roi_valid_trace.shape[0], :], \n",
    "                                                          roi_valid_trace, offset_cluster_combine)\n",
    "        #         print(\"%s %s\" % (train, test))\n",
    "        \n",
    "    if n_downsample == 1:\n",
    "        fl.save(\"regularization_costs.h5\", dict(costs=costs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_within_std_err(cell_costs):\n",
    "    mean_cost = np.nanmean(cell_costs, 1)\n",
    "    std_err_cost = np.nanstd(cell_costs, 1) / np.sqrt(np.sum(~np.isnan(cell_costs[0, :])) - 1)\n",
    "\n",
    "    min_idx = np.argmin(mean_cost)\n",
    "    std_err_min = std_err_cost[min_idx]\n",
    "\n",
    "    i = min_idx\n",
    "    while i < len(mean_cost) - 1 and mean_cost[i + 1] < mean_cost[min_idx] + std_err_min:\n",
    "        i += 1\n",
    "    \n",
    "    if i != len(mean_cost) and mean_cost[i] < 1:\n",
    "        return i\n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,3))\n",
    "i = 21\n",
    "i *= 30\n",
    "mean_err = np.nanmean(costs[i, :, :], 1)\n",
    "std_err = np.nanstd(costs[i, :, :], 1) / np.sqrt(np.sum(~np.isnan(costs[i, 0, :])) - 1)\n",
    "print(mean_err + std_err)\n",
    "\n",
    "= max_within_std_err(costs[i, :, :])\n",
    "if ~np.isnan(threshold):\n",
    "    plt.axvline(threshold, c=(0.3,)*3)\n",
    "    plt.axhline(mean_err[threshold],  c=(0.3,)*3)\n",
    "#plt.fill_between(np.log10(reg_lambda_arr), mean_err-std_err, mean_err+std_err, facecolor=sns.color_palette()[0], \n",
    "#                 alpha=0.4, edgecolor=\"None\")\n",
    "#plt.plot(np.log10(reg_lambda_arr), mean_err, color=sns.color_palette()[0])\n",
    "plt.fill_between(np.arange(len(reg_lambda_arr)), mean_err-std_err, mean_err+std_err, facecolor=sns.color_palette()[0], \n",
    "                 alpha=0.4, edgecolor=\"None\")\n",
    "plt.plot(mean_err, color=sns.color_palette()[0])\n",
    "\n",
    "plt.xlabel(\"Regularization coeff.\")\n",
    "plt.ylabel(\"Mean MSE\")\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_lambda_reg = reg_lambda_arr[11]  # reg_coefs[np.argmin(np.nanmean(costs, 1))]\n",
    "# final_lambda_reg = 0.00031  # from long fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc_regs = np.arange(n_gc_clust)\n",
    "io_regs = np.arange(n_io_clust)+n_gc_clust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(n_components, n_components, sharex=True, sharey=True)\n",
    "\n",
    "for i in range(n_components):\n",
    "    for j in range(n_components):\n",
    "        if j>=i:\n",
    "            axes[i, j].axis('off')\n",
    "        else:\n",
    "            for gc_reg in gc_regs:\n",
    "                axes[i, j].plot([0, regressors_pca[gc_reg, j]], [0, regressors_pca[gc_reg, i]], c=sns.color_palette()[0])\n",
    "            for io_reg in io_regs:\n",
    "                axes[i, j].plot([0, regressors_pca[io_reg, j]], [0, regressors_pca[io_reg, i]], c=sns.color_palette()[1])\n",
    "            axes[i, j].set_xticks([])\n",
    "            axes[i, j].set_yticks([])\n",
    "            axes[i, j].axvline(0, ls='--', c='white', alpha=.2)\n",
    "            axes[i, j].axhline(0, ls='--', c='white', alpha=.2)\n",
    "        \n",
    "for i in range(n_components):\n",
    "    axes[i, 0].set_ylabel('PC{}'.format(i+1))\n",
    "    \n",
    "for j in range(n_components):\n",
    "    axes[-1, j].set_xlabel('PC{}'.format(j+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressors_pca[gc_regs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc_mean = np.nanmean(regressors_pca[gc_regs], 0)\n",
    "gc_std = np.nanstd(regressors_pca[gc_regs], 0)\n",
    "gc_std_norm = gc_std/np.abs(gc_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressors_pca[io_regs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "io_mean = np.nanmean(regressors_pca[io_regs], 0)\n",
    "io_std = np.nanstd(regressors_pca[io_regs], 0)\n",
    "io_std_norm = io_std/np.abs(io_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traces_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "io_std_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "io_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(regressors_mat[3, :])\n",
    "\n",
    "plt.plot(pca.inverse_transform(regressors_pca[3, :]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the traces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the traces with the specified lambda:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "costs_final = np.full(n_rois, np.nan)  # Costs of our final fit\n",
    "coefs_final = np.full((n_rois, regressors_mat.shape[0] + 1), np.nan)  # Coefs from our final fit\n",
    "\n",
    "\n",
    "valid_roi_idxs = []\n",
    "for i_roi in range(n_rois):\n",
    "    if np.mod(i_roi, 100) == 0:\n",
    "        print(i_roi)\n",
    "    cost_idx = max_within_std_err(costs[i_roi, :, :])\n",
    "    \n",
    "    if not np.isnan(cost_idx):\n",
    "        final_lambda_reg = reg_lambda_arr[cost_idx]\n",
    "        \n",
    "        # Get test and fit indexes and traces:\n",
    "        roi_test_idxs = test_idxs[i_roi]\n",
    "        roi_test_trace = clean_traces[i_roi, :,roi_test_idxs].flatten()\n",
    "\n",
    "        roi_fit_idxs = fit_idxs[i_roi]\n",
    "        roi_fit_trace = clean_traces[i_roi, :, roi_fit_idxs].flatten()  \n",
    "\n",
    "        # Fit the train set:\n",
    "        res = optimize.minimize(minimization_func, method='SLSQP', \n",
    "                                args=(regressors_concat[:roi_fit_trace.shape[0], :], \n",
    "                                      roi_fit_trace, offset_cluster_combine, \n",
    "                                      cost_func, reg_func, 0)\n",
    "                                ,\n",
    "                                x0=coefs_init_guess, bounds=coefs_bounds)\n",
    "\n",
    "        # Calculate cost on the test set:\n",
    "        costs_final[i_roi] = cost_func(res.x, regressors_concat[:roi_test_trace.shape[0], :], \n",
    "                                                  roi_test_trace, offset_cluster_combine)\n",
    "        # Save the coefficients:\n",
    "        coefs_final[i_roi, :] = res.x\n",
    "        \n",
    "        valid_roi_idxs.append(i_roi)\n",
    "        \n",
    "valid_roi_idxs = np.array(valid_roi_idxs)\n",
    "n_valid_rois = len(valid_roi_idxs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep only properly fit rois data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs_final_sel = coefs_final[valid_roi_idxs, :]\n",
    "costs_final_sel = costs_final[valid_roi_idxs]\n",
    "clean_traces_sel = clean_traces[valid_roi_idxs, :, :]\n",
    "\n",
    "test_idxs_sel = [test_idxs[i] for i in valid_roi_idxs]\n",
    "fit_idxs_sel = [fit_idxs[i] for i in valid_roi_idxs]\n",
    "rel_idx_sel = data_dict[\"PC\"][\"rel_idxs\"][valid_roi_idxs]\n",
    "clust_lab_sel = data_dict[\"PC\"][\"clust_labels\"][valid_roi_idxs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit with shuffled weights to decide what is a \"random\" fit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs_shuf = np.empty_like(coefs_final_sel)\n",
    "\n",
    "for i in range(coefs_final_sel.shape[1]):\n",
    "    shuf_idx = np.random.permutation(coefs_final_sel.shape[0])\n",
    "    coefs_shuf[:, i] = coefs_final_sel[shuf_idx, i]\n",
    "    \n",
    "costs_shuf = np.full(n_valid_rois, np.nan)  # Costs from shuffled weights\n",
    "\n",
    "for i_roi in range(n_valid_rois):\n",
    "    \n",
    "    # Get test and fit indexes and traces:\n",
    "    roi_test_idxs = test_idxs_sel[i_roi]\n",
    "    roi_test_trace = clean_traces_sel[i_roi, :,roi_test_idxs].flatten()\n",
    "        \n",
    "    # Calculate cost on the test set with shuffled weights:\n",
    "    costs_shuf[i_roi] = cost_func(coefs_shuf[i_roi, :], regressors_concat[:roi_test_trace.shape[0], :], \n",
    "                                              roi_test_trace, offset_cluster_combine)\n",
    "    \n",
    "cost_threshold = np.percentile(costs_shuf, 5)\n",
    "sel_fit = np.argwhere(costs_final_sel < cost_threshold)[:, 0]\n",
    "n_valid_rois = len(sel_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs_final_sel = coefs_final_sel[sel_fit, :]\n",
    "costs_final_sel = costs_final_sel[sel_fit]\n",
    "clean_traces_sel = clean_traces_sel[sel_fit, :, :]\n",
    "\n",
    "test_idxs_sel = [test_idxs_sel[i] for i in sel_fit]\n",
    "fit_idxs_sel = [fit_idxs_sel[i] for i in sel_fit]\n",
    "rel_idx_sel = rel_idx_sel[sel_fit]\n",
    "clust_lab_sel = clust_lab_sel[sel_fit]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_figure(costs_final, costs_shuf, cost_threshold, figure=None, frame=None):\n",
    "    \n",
    "    bin_array = np.arange(0.01, 1.6, 0.05)\n",
    "    costs_shuf_hist, b = np.histogram(costs_shuf, bin_array)\n",
    "    costs_final_hist, b = np.histogram(costs_final, bin_array)\n",
    "    costs_final_sel_hist, b = np.histogram(costs_final[costs_final < cost_threshold], bin_array)\n",
    "    \n",
    "    if figure is None:\n",
    "        figure = plt.figure(figsize=(3.,2))\n",
    "    \n",
    "    a = 0.7\n",
    "    ax_coefs = add_offset_axes(figure, (0.2, 0.2, 0.7, 0.7), frame=frame)\n",
    "    \n",
    "    x = (bin_array[1:] + bin_array[:-1]) / 2\n",
    "    ax_coefs.fill_between(x, costs_shuf_hist, step=\"mid\", alpha=a, edgecolor=None, facecolor=sns.color_palette()[1])\n",
    "    ax_coefs.fill_between(x, costs_final_sel_hist, step=\"mid\", alpha=a, edgecolor=None, facecolor=sns.color_palette()[0])\n",
    "    ax_coefs.step(x, costs_final_hist, where=\"mid\", alpha=a, color=sns.color_palette()[0])\n",
    "    \n",
    "    ax_coefs.axvline(cost_threshold, c=(0.4,)*3)\n",
    "    ax_coefs.set_xlabel(\"Cost (on test)\")\n",
    "    ax_coefs.set_ylabel(\"Count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_figure(costs_final, costs_shuf, cost_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_figure(costs_final, costs_shuf, cost_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.argsort(costs_final)[0]\n",
    "\n",
    "ylim = 2.5\n",
    "coefs = coefs_final[idx, :]\n",
    "cellmat = traces[idx, :, :]\n",
    "roi_test_idxs = test_idxs[idx]\n",
    "test = clean_traces[idx, :,roi_test_idxs]\n",
    "    \n",
    "roi_fit_idxs = fit_idxs[idx]\n",
    "fit = clean_traces[idx, :, roi_fit_idxs]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.subplot(2,1,1)\n",
    "all_resp = fit\n",
    "# reshaped = all_resp.reshape(all_resp.shape[0] // cellmat.shape[0], cellmat.shape[0])\n",
    "plt.plot(all_resp.T, color = (0.6,)*3, linewidth=0.6)\n",
    "plt.plot(np.mean(all_resp, 0))\n",
    "plt.plot(nanzscore(offset_cluster_combine(coefs, regressors_mat.T)))\n",
    "plt.ylim(-ylim, ylim)\n",
    "plt.ylabel(\"Train set\")\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "all_resp = test\n",
    "# reshaped = all_resp.reshape(all_resp.shape[0] // cellmat.shape[0], cellmat.shape[0])\n",
    "plt.plot(all_resp.T, color = (0.6,)*3, linewidth=0.6)\n",
    "plt.plot(np.mean(all_resp, 0))\n",
    "plt.plot(nanzscore(offset_cluster_combine(coefs, regressors_mat.T)))\n",
    "plt.ylim(-ylim, ylim)\n",
    "plt.ylabel(\"Test set\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coefs_plot(clust_lab, coefs_final, figure=None, frame=None):\n",
    "    if figure is None:\n",
    "        figure = plt.figure(figsize=(4, 3))\n",
    "    \n",
    "    idxs_sort = np.argsort(clust_lab)\n",
    "\n",
    "    c_lim = 100\n",
    "    ax_coefs = add_offset_axes(figure, (0.05, 0.2, 0.75, 0.8), frame=frame)\n",
    "    im = ax_coefs.imshow(coefs_final[idxs_sort, 1:].T, vmin=0, vmax=c_lim, aspect=\"auto\", cmap=\"Reds\")\n",
    "    ax_coefs.set_xlabel(\"Roi n.\")\n",
    "    ax_coefs.axhline(7.5, c=(0.3,)*3)\n",
    "    ax_coefs.set_yticks([3., 10.])\n",
    "    ax_coefs.set_ylim([13.5, -0.5])\n",
    "    ax_coefs.tick_params(length=0)\n",
    "    ax_coefs.set_yticklabels([\"$w_{GC}$\", \"$w_{ION}$\"], rotation=90)\n",
    "    [ax_coefs.axes.spines[s].set_visible(False) for s in\n",
    "         [\"left\", \"right\", \"top\", \"bottom\"]]\n",
    "    for ytick, color in zip(ax_coefs.get_yticklabels(), sns.color_palette()[:2]):\n",
    "        ytick.set_color(color)\n",
    "\n",
    "    k = np.sum(clust_lab == 0)\n",
    "    for n in range(1, clust_lab.max() + 1): \n",
    "        ax_coefs.axvline(k, c=(0.3,)*3)\n",
    "        k += np.sum(clust_lab == n)\n",
    "\n",
    "\n",
    "    axcolor = add_offset_axes(figure, [0.86, 0.2, 0.02, 0.12], frame=frame)\n",
    "    cbar = plt.colorbar(im, cax=axcolor, orientation=\"vertical\")\n",
    "    cbar.set_ticks([0, c_lim])\n",
    "    cbar.ax.tick_params(length=3)\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs_plot(clust_lab_sel, coefs_final_sel, figure=None, frame=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check error histogram and relationship with cell reliability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check error of the fit as a function of the cell reliability index. We expect a negative relationship:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_vs_reliabil(rel_idx, costs_final, figure=None, frame=None):\n",
    "    \n",
    "    \n",
    "    \n",
    "    if figure is None:\n",
    "        figure = plt.figure(figsize=(3,2))\n",
    "        \n",
    "    ax = add_offset_axes(figure, (0.2, 0.2, 0.7, 0.7), frame=frame)\n",
    "    ax.scatter(rel_idx, costs_final, s=5)\n",
    "    ax.set_xlabel(\"Reliability idx\")\n",
    "    ax.set_ylabel(\"Fit error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_vs_reliabil(rel_idx_sel, costs_final_sel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at distribution of IO and GC coefficients weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_coefs = coefs_final_sel.copy()\n",
    "cleaned_coefs = cleaned_coefs[~(cleaned_coefs == 0).all(1), :]\n",
    "coefs_sum = np.nansum(np.abs(cleaned_coefs), 1)\n",
    "norm_coefs = (cleaned_coefs.T / coefs_sum).T\n",
    "non_zero_coefs = np.abs(norm_coefs) > 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gc_io_weights_hist(cleaned_coefs, figure=None, frame=None):\n",
    "    if figure is None:\n",
    "        figure = plt.figure(figsize=(3,2))\n",
    "        \n",
    "    ax = add_offset_axes(figure, (0.2, 0.2, 0.7, 0.7), frame=frame)\n",
    "    l=400\n",
    "    ax.hist(cleaned_coefs[:, 1:9].sum(1), np.arange(-l, l, 20), alpha=0.6, label=\"GC\")\n",
    "    ax.hist(cleaned_coefs[:, 9:].sum(1), np.arange(-l, l, 20), alpha=0.6, label=\"IO\")\n",
    "    ax.legend()\n",
    "    ax.set_xlabel(\"Weight of coefs\")\n",
    "    ax.set_ylabel(\"Count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc_io_weights_hist(cleaned_coefs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at distribution of number of coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gc_io_nonzero_hist(non_zero_coefs, figure=None, frame=None):\n",
    "    if figure is None:\n",
    "        figure = plt.figure(figsize=(3,2))\n",
    "        \n",
    "    ax = add_offset_axes(figure, (0.2, 0.2, 0.7, 0.7), frame=frame)\n",
    "    ax.hist(non_zero_coefs[:, 1:9].sum(1), np.arange(0, 15), \n",
    "            label=\"GC (mn={:2.1f})\".format(np.nanmean(non_zero_coefs[:, 1:9].sum(1))), alpha=0.6)\n",
    "    ax.hist(non_zero_coefs[:, 9:].sum(1), np.arange(0, 15), \n",
    "            label=\"IO (mn={:2.1f})\".format(np.nanmean(non_zero_coefs[:, 9:].sum(1))), alpha=0.6)\n",
    "    ax.legend()\n",
    "    ax.set_xlabel(\"N. of coefs\")\n",
    "    ax.set_ylabel(\"Count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc_io_nonzero_hist(non_zero_coefs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GC-IO index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create \"GC vs IO-ness index\", looking at the ratio of coefficients of GC clusters and IO clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_gc = np.abs(coefs_final_sel[:, 1:n_gc_clust+1]).mean(1)\n",
    "w_io = np.abs(coefs_final_sel[:, n_gc_clust+1:]).mean(1)\n",
    "gc_io_idx = (w_gc - w_io) / (w_gc + w_io)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a IO-GC colormap\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "colors = [sns.color_palette()[1], (.72,)*3, sns.color_palette()[0]]\n",
    "n_bins = 200\n",
    "cmap_name = 'contributions_cmap'\n",
    "custom_cmap = LinearSegmentedColormap.from_list(cmap_name, colors, N=n_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexes_plot(gc_io_idx, rel_idxs, figure=None, frame=None):\n",
    "    r = stats.spearmanr(gc_io_idx[~np.isnan(gc_io_idx)], rel_idxs[~np.isnan(gc_io_idx)])\n",
    "    \n",
    "    \n",
    "    if figure is None:\n",
    "        figure = plt.figure(figsize=(3., 2.))\n",
    "    \n",
    "    ax = add_offset_axes(figure, (0.2, 0.2, 0.7, 0.7), frame=frame)\n",
    "    ax.axvline(0, c = (0.6, )*3, zorder=-100)\n",
    "    ax.scatter(gc_io_idx, rel_idxs, c=gc_io_idx, cmap=custom_cmap, s=6, vmin=-1, vmax=1, \n",
    "               edgecolor=(0.3,)*3, linewidths=0.1)\n",
    "    ax.set_xlim(-1.1, 1.1)\n",
    "    ax.set_ylabel(\"Reliability index\")\n",
    "    ax.set_xlabel(\"GC/IO idx\")\n",
    "#     numbers = \n",
    "    ax.text(-1, 0.7, \"$\\\\rho$\" + \"={:1.2f} \\np={:1.2}\".format(r.correlation, r.pvalue), fontsize=7, color=(0.3,)*3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes_plot(gc_io_idx, rel_idx_sel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single cell example fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "11, 17, 32, 33, (44), 71\n",
    "\n",
    "37, 60, 68"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_single_cell_plot(traces, all_coefs, roi_test_idxs, idx, \n",
    "                          legend=True, bar=True, figure=None, frame=None):\n",
    "    if figure is None:\n",
    "        figure = plt.figure(figsize=(7, 2))\n",
    "    coefs = all_coefs[idx, :]\n",
    "    roi_test_idxs = test_idxs[idx]\n",
    "    reshaped = traces[idx, :, roi_test_idxs]\n",
    "    \n",
    "    ylim = 3.7\n",
    "    axtrace = add_offset_axes(figure, (0.0, 0.1, 0.58, 0.92), frame=frame)\n",
    "    axtrace.plot(stim[:, 0], reshaped.T, color=sns.color_palette()[2], linewidth=0.5)\n",
    "    axtrace.plot(stim[:, 0], np.nanmean(reshaped, 0), color=sns.color_palette()[2], linewidth=2, label=\"PC trace\")\n",
    "    axtrace.plot(stim[:, 0], nanzscore(offset_cluster_combine(coefs, regressors_mat.T)), color=\"k\", label=\"Fit\")\n",
    "    shade_plot(stim, axtrace, shade_range=(0.75, 0.98))\n",
    "    \n",
    "    axtrace.set_ylim(-ylim, ylim)\n",
    "    axtrace.set_xlim(0, stim[-1, 0])\n",
    "    axtrace.spines[\"left\"].set_visible(False)\n",
    "    axtrace.set_yticks([])\n",
    "    if bar:\n",
    "        make_bar(axtrace, (stim[-1, 0]-10, stim[-1, 0]), label=\"10 s\")\n",
    "    else:\n",
    "        axtrace.spines[\"bottom\"].set_visible(False)\n",
    "        axtrace.set_xticks([])\n",
    "    \n",
    "    # Legend\n",
    "    handles, labels = axtrace.get_legend_handles_labels()\n",
    "    unique = [(h, l) for i, (h, l) in enumerate(zip(handles, labels)) if l not in labels[:i]]\n",
    "    if legend:\n",
    "        plt.legend(*zip(*unique), loc=\"lower left\", fontsize=7)\n",
    "    \n",
    "    # Inserts\n",
    "    ylim = 1.5\n",
    "    y_off = -0.5\n",
    "    coefs_gc = np.zeros(coefs.shape)\n",
    "    coefs_io = np.zeros(coefs.shape)\n",
    "    coefs_gc[1:9] = coefs[1:9]\n",
    "    coefs_io[9:] = coefs[9:]\n",
    "    for n, (ax_pos, lab, c_coefs) in enumerate(zip([(0.6, 0.6, 0.4, 0.4), (0.6, 0.1, 0.4, 0.4)], \n",
    "                                                   [\"GC\", \"IO\"],\n",
    "                                                   [coefs_gc, coefs_io])):\n",
    "        col = sns.color_palette()[n]\n",
    "        ax_little = add_offset_axes(figure, ax_pos, frame=frame)\n",
    "        ax_little.plot(stim[:, 0], offset_cluster_combine(c_coefs, regressors_mat.T), color=col)\n",
    "        shade_plot(stim, ax_little, shade_range=(0.75, 0.98))\n",
    "        ax_little.set_ylim(-ylim-y_off, ylim-y_off)\n",
    "        ax_little.axis(\"off\")\n",
    "        \n",
    "        if legend:\n",
    "            ax_little.text(2, -ylim-y_off + 0.2, lab + \" contribution\", color=col, fontsize=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cells  # (gc_io idx sort): \n",
    "- 97, 254 example IO\n",
    "- 197, example comb\n",
    "- 89, 115, 60 example comb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "97, 197, 89"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel = costs_final_sel < 0.9\n",
    "idx = np.argsort(gc_io_idx[sel])[-11]\n",
    "make_single_cell_plot(clean_traces_sel, coefs_final_sel, fit_idxs_sel, idx, figure=None, frame=None)\n",
    "# for idx in [98]:\n",
    "#     make_single_cell_plot(clean_traces_sel, coefs_final_sel, fit_idxs_sel, idx, figure=None, frame=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time contribution plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_contributions_mat = np.zeros((n_rep_timepts, n_valid_rois))\n",
    "coefs_gc = np.zeros(coefs_final_sel.shape)\n",
    "coefs_io = np.zeros(coefs_final_sel.shape)\n",
    "coefs_gc[:, 1:9] = coefs_final_sel[:, 1:9]\n",
    "coefs_io[:, 9:] = coefs_final_sel[:, 9:]\n",
    "\n",
    "for i_roi in range(n_valid_rois):\n",
    "       \n",
    "    gc_contribution = offset_cluster_combine(coefs_gc[i_roi, :], regressors_mat.T)\n",
    "    io_contribution = offset_cluster_combine(coefs_io[i_roi, :], regressors_mat.T)\n",
    "    \n",
    "    contribution_ratio = (np.abs(gc_contribution) - np.abs(io_contribution)) / \\\n",
    "                (np.abs(gc_contribution) + np.abs(io_contribution))\n",
    "    \n",
    "    time_contributions_mat[:, i_roi] = contribution_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_contr_plot(time_contributions_mat, stim, figure=None, frame=None):\n",
    "    \n",
    "    if figure is None:\n",
    "        figure = plt.figure(figsize=(7, 1.5))\n",
    "    \n",
    "    ax = add_offset_axes(figure, (0.1, 0.1, 0.9, 0.9), frame=frame)\n",
    "\n",
    "    # Find quartiles:\n",
    "    median_contr = np.nanmedian(time_contributions_mat, 1)\n",
    "    low_quart_contr = np.nanquantile(time_contributions_mat, 0.25, axis=1)\n",
    "    high_quart_contr = np.nanquantile(time_contributions_mat, 0.75, axis=1)\n",
    "    \n",
    "    # Plot trace and filling:\n",
    "    i_col=5\n",
    "    ax.plot(stim[:, 0], median_contr, color=sns.color_palette()[i_col])\n",
    "    ax.fill_between(stim[:,0], low_quart_contr, high_quart_contr, facecolor=sns.color_palette()[i_col], \n",
    "                         alpha=.3, zorder=100, edgecolor=None)\n",
    "    shade_plot(stim, ax, shade_range=(0.75, 0.98))\n",
    "    ax.axhline(0, c = (0.3,)*3, zorder=1)\n",
    "    ax.set_ylim(-1., 1.)\n",
    "    ax.set_xlim(0, stim[-1, 0])\n",
    "#     for y_line in [-1, 1]:\n",
    "#         ax.axhline(y_line, color=(0.3,)*3, linewidth=0.5)\n",
    "    \n",
    "    make_bar(ax, (stim[-1, 0]-10, stim[-1, 0]), label=\"10 s\")\n",
    "    ax.set_ylabel('Contribution ratio')\n",
    "    ax.text(0.5, 1.05, 'GC contributions dominates', fontsize=7, color=sns.color_palette()[0])\n",
    "    ax.text(0.5, -1.15, 'IO contributions dominates', fontsize=7, color=sns.color_palette()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_contr_plot(time_contributions_mat, stim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assemble final panel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.lines as lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clust_names = [\"IO{}\".format(i+1) for i in range(n_io_clust)] + [\"GC{}\".format(i+1) for i in range(n_gc_clust)]\n",
    "cols = [sns.color_palette()[1]]*n_io_clust + [sns.color_palette()[0]]*n_gc_clust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the regressor panel:\n",
    "def reg_panel_plot(regressors_mat, figure=None, ax=None, frame=None):\n",
    "    if figure is None:\n",
    "        figure = plt.figure(figsize=(3, 3))\n",
    "\n",
    "    offset = 0.01\n",
    "    if ax is None:\n",
    "        ax = add_offset_axes(figure, (0., 0., 1, 1), frameon=False, frame=frame)\n",
    "    cols = sns.color_palette()\n",
    "    for i, col in enumerate([cols[0], ]*n_gc_clust + [cols[1], ]*n_io_clust):\n",
    "        ax.fill_between(stim[:, 0], np.zeros(stim[:, 0].shape) - i*offset, \n",
    "                        regressors_mat[i, :] - i*offset, color=col)\n",
    "        \n",
    "    ax.axis(\"off\")\n",
    "    \n",
    "# Plot a trace panel:\n",
    "def little_trace_plot(clean_traces, i=0, figure=None, ax=None, frame=None):\n",
    "    if figure is None:\n",
    "        figure = plt.figure(figsize=(3, 1))\n",
    "\n",
    "    if ax is None:\n",
    "        ax = add_offset_axes(figure, (0., 0., 1, 1), frameon=False, frame=frame)\n",
    "    offset = 0.01\n",
    "    ax.plot(clean_traces[i, :, :1].T.flatten(), c=sns.color_palette()[2])\n",
    "    ax.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def schema_panel(clean_traces, regressors_mat, i=9, figure=None, frame=None):\n",
    "    if figure is None:        \n",
    "        figure = plt.figure(figsize=(7, 3))\n",
    "\n",
    "    schema_y = 0.3\n",
    "\n",
    "    h_cent = 0.5\n",
    "    trace_h = 0.2\n",
    "    reg_h = 0.9\n",
    "    schema_h = 0.9\n",
    "    ax_trace = add_offset_axes(figure, [0.7, h_cent - trace_h/2, 0.3, trace_h], frameon=False, frame=frame)\n",
    "    ax_regressors = add_offset_axes(figure, [0.02, h_cent - reg_h/2 - 0.01, 0.3, reg_h], frameon=False, frame=frame)\n",
    "    ax_schema = add_offset_axes(figure, [0.31, h_cent - schema_h/2, 0.4, schema_h], frameon=False, aspect=1., frame=frame)\n",
    "    little_trace_plot(clean_traces, i=18, figure=figure, ax=ax_trace)\n",
    "\n",
    "    reg_panel_plot(regressors_mat, figure=figure, ax=ax_regressors)\n",
    "\n",
    "    ax_schema.xaxis.set_visible(False)\n",
    "    ax_schema.yaxis.set_visible(False)\n",
    "\n",
    "    n_clust = 14\n",
    "    n_gc_clust\n",
    "    n_io_clust\n",
    "    c_cent = (0.6, 0.5)  # x, y\n",
    "    p = mpatches.Circle(c_cent, 0.1, edgecolor=\"k\", facecolor=\"w\")\n",
    "\n",
    "    w_pos_x = 0.0\n",
    "    line_displ_x = 0.2\n",
    "    for i_clust, (label, col) in enumerate(zip(clust_names, cols)):\n",
    "        c = 1/(2*(n_clust + 1)) + i_clust/(n_clust + 1)\n",
    "        l = lines.Line2D([w_pos_x + line_displ_x, c_cent[0]], [c, c_cent[1]], c=\"k\", zorder=-100)\n",
    "        ax_schema.add_line(l)\n",
    "        ax_schema.text(w_pos_x, c, \"$\\cdot w_i^{\" + label + \"}$\", ha=\"left\", va=\"center\", fontsize=7, color=col)\n",
    "\n",
    "    c = 1/(2*n_clust) + (i_clust + 1)/(n_clust + 1)\n",
    "    l = lines.Line2D([w_pos_x + line_displ_x, c_cent[0]], [c, c_cent[1]], c=\"k\", zorder=-100)\n",
    "    ax_schema.add_line(l)\n",
    "    ax_schema.text(w_pos_x, c, \"$offset_i$\", ha=\"left\", va=\"center\", fontsize=7, color=\"k\")\n",
    "\n",
    "    ax_schema.add_patch(p)\n",
    "\n",
    "    l = lines.Line2D([c_cent[0], 1], [c_cent[1], c_cent[1]], c=\"k\", zorder=-100)\n",
    "    ax_schema.add_line(l)\n",
    "\n",
    "    # Text\n",
    "    ax_schema.text(*c_cent, \"$\\sum$\", ha=\"center\", va=\"center\", fontsize=9)\n",
    "    ax_schema.text(0.55, 0.9, \"$PC_i = regr^{GC} \\cdot w_i^{GC} + regr^{IO} \\cdot w_i^{IO} + offset_i$\", \n",
    "                   ha=\"left\", va=\"center\", fontsize=7.5)\n",
    "    ax_trace.text(0, -3, \"$PC_i$\", ha=\"left\", va=\"center\", fontsize=9, color=sns.color_palette()[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(7, 9))\n",
    "schema_panel(clean_traces, regressors_mat, i=9, figure=fig, frame=[0.25, 0.77, 0.6, 0.2])\n",
    "\n",
    "offset_y = 0.13\n",
    "start_y = 0.36\n",
    "for i, idx in enumerate([97, 197, 89]):\n",
    "    y = start_y + i*offset_y\n",
    "    \n",
    "    make_single_cell_plot(clean_traces_sel, coefs_final_sel, fit_idxs_sel, idx, figure=fig, frame=[0., y, 0.53, 0.12], legend=(i==2), bar=(i==0))\n",
    "\n",
    "coefs_plot(clust_lab_sel, coefs_final_sel, figure=fig, frame=[0.58, 0.56, 0.4, 0.18])\n",
    "indexes_plot(gc_io_idx, rel_idx_sel, figure=fig, frame=[0.58, 0.35, 0.35, 0.2])\n",
    "time_contr_plot(time_contributions_mat, stim, figure= fig, frame=[0.1, 0.17, 0.65, 0.15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if fig_fold is not None:\n",
    "    fig.savefig(fig_fold / \"fig7_panel.pdf\", forma=\"pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supplementary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figsupp = plt.figure(figsize=(6, 4))\n",
    "s = 0.45\n",
    "cost_figure(costs_final, costs_shuf, cost_threshold, figure=figsupp, frame=[0., 0.5, s, s])\n",
    "error_vs_reliabil(rel_idx_sel, costs_final_sel, figure=figsupp, frame=[0.5, 0.5, s, s])\n",
    "# gc_io_weights_hist(cleaned_coefs, figure=figsupp, frame=[0., 0., s, s])\n",
    "gc_io_nonzero_hist(non_zero_coefs, figure=figsupp, frame=[0.25, 0., s, s])\n",
    "# sorted_examples_plot(coefs_final, gc_io_idx, clean_traces, test_idxs, figure=figure6supp, frame=[0.5, 0.1, 0.5, 0.8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if fig_fold is not None:\n",
    "    figsupp.savefig(fig_fold.parent.parent / (fig_fold.parent.name + \"supp/src\") / \"supp_panel.pdf\", format=\"pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
